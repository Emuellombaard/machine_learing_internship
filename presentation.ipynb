{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effect of Quantization on Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are we aiming for?\n",
    "\n",
    "Our aim is to build a program which quantizes a neural network and, if time allows us, to try and compress the network. Quantizing has a minimal effect on the accuracy of the model. Our quantizer will decrease the storage space and memory usage of the pre-trained model and hopefully increase the running speed.\n",
    "\n",
    "Our method is based on the paper Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights, by Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen. It uses group-wise weight quantization and weight partitioning to do the quantization. It converts a pre-trained full precision convolutional neural network model into a low precision model, where the weight are defined by powers of either 2 or zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will look at how quantization affects the following three factors:\n",
    " - Decreased storage space usage\n",
    " - Decreased memory usage\n",
    " - Increased running speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first ...\n",
    "\n",
    "Lets import some stuff..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.metrics import dice\n",
    "from fastai.vision import get_transforms, SegmentationItemList, to_device, Learner,  validate\n",
    "from zb_hps_algo.pytorch_models.models.unet import Unet\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import tester #our little tester module\n",
    "from quantization import INQScheduler #the quantizer object\n",
    "import quantization #the quantization module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading up a dataset and some pretrained models for later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "img_size = 256\n",
    "zb_dataset_path = Path('/data/home/craig/zb-datasets/')\n",
    "training_path = zb_dataset_path / 'nn_training' / 'head_finder' / f'{img_size}'\n",
    "model_save_dir = Path('models/')\n",
    "\n",
    "classes = ['Background', 'Right', 'Left']\n",
    "tfms = get_transforms(do_flip=False, flip_vert=False,\n",
    "                      p_lighting=0.5, max_lighting=0.85,\n",
    "                      max_rotate=20,\n",
    "                      max_warp=0,\n",
    "                      max_zoom=1.2)\n",
    "data = (SegmentationItemList.from_folder(training_path / 'data', convert_mode='L')#, div=False)\n",
    "        .split_by_rand_pct(seed=42)\n",
    "        .label_from_func(lambda x: training_path / 'labels' / x.name, classes=classes)\n",
    "        .transform(tfms, tfm_y=True, size=img_size)\n",
    "        .databunch(bs=6)  # 4@512 and 12@256 will just about max out a rtx2070\n",
    "        .normalize())  # This uses a batch's statistics to normalize the dataset. I don't like this.\n",
    "\n",
    "\n",
    "model = Unet(1, 3)\n",
    "model.load_state_dict(torch.load(model_save_dir / \"model.tmp\"))\n",
    "model.eval()\n",
    "learner = Learner(data, model, metrics=[dice], opt_func=torch.optim.SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Quantizer works\n",
    "we want to convert all the weights to be represented by some integer *__p__* such that $2^{p}$ is the closest approximation of the weight as possible.\n",
    "\n",
    "To minimise the loss in accuracy we quantize the weights in increments. After each increment we re-train the model  so that the network can compensate for the loss in accuracy.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    ">`Instantiate quantizer\n",
    "For (number of increments - 1):\n",
    "    Step quantizer for next increment\n",
    "    Train network\n",
    "Step quantizer for final increment` <br>\n",
    "\n",
    "\n",
    "Using this algorithm we can specify how much we want to quantize in each step with the increment, allowing for fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "increments = [0.5,0.75,0.82,1.0] #We define the increments, noting that the last increment must ALWAYS be one\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3) #We instantiate an optimizer\n",
    "\n",
    "quantizer = INQScheduler(optimizer, increments) #Instantiating quantizer which loops\n",
    "                                                #through the network in the pre-defined increments\n",
    "\n",
    "for i in range(len(increments)):\n",
    "    quantizer.step()\n",
    "    learner.fit(5, lr = 1e-3)\n",
    "inq_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our model now quantized, we can now compare the quantized model to the original model and assess its performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unquantized loss:\t0.0391\n",
      "Quantized Loss:\t\t0.0408\n",
      "Percentage Increase:\t4.2370%\n"
     ]
    }
   ],
   "source": [
    "#Loading in our pre-quantized model\n",
    "quant   = Unet(1, 3)\n",
    "quant.load_state_dict(torch.load(model_save_dir / \"model_quant.tmp\"))\n",
    "quant.eval()\n",
    "quant_learner = Learner(data, quant, metrics=[dice], opt_func=torch.optim.SGD)\n",
    "\n",
    "#Loading in our pre-trained model\n",
    "unquant = Unet(1, 3)\n",
    "unquant.load_state_dict(torch.load(model_save_dir / \"model.tmp\"))\n",
    "unquant.eval()\n",
    "unquant_learner = Learner(data, unquant, metrics=[dice], opt_func=torch.optim.SGD)\n",
    "\n",
    "quant.to(device)\n",
    "unquant.to(device)\n",
    "\n",
    "tester.compare_loss(quant_learner,unquant_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size\n",
    "One can drastically decrease physical size of the network if we rather save our integer *__p__* instead of a float for every single weight and then convert all of those saved integers back into floats using $2^{p}$ to recover the model ready to be run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we load up our quantized model\n",
    "model = Unet(1, 3)\n",
    "model.load_state_dict(torch.load(model_save_dir / \"model_quant.tmp\"))\n",
    "model.eval()\n",
    "\n",
    "#We create a quantizer for it and have it turn the weights into exponent integers\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)\n",
    "quantizer = INQScheduler(optimizer, [1]) #Constructing the quantizer\n",
    "quantizer.quantize_int()\n",
    "\n",
    "#Now we can save the model, doing so will also\n",
    "quantization.save_quantized_model(model, model_save_dir / \"saved_qaunt.tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model loss:\t0.0408\n",
      "Loaded model loss:\t0.0408\n",
      "\n",
      "Unqauntized Model Size:\t62Mb\n",
      "Quantized Model Size:\t15.55Mb\n",
      "Quantized size is 24.99% of the unquantized size\n"
     ]
    }
   ],
   "source": [
    "model = Unet(1, 3)\n",
    "model.load_state_dict(torch.load(model_save_dir / \"model_quant.tmp\"))\n",
    "model.eval()\n",
    "\n",
    "loaded_model = Unet(1, 3)\n",
    "quantization.load_quantized_model(loaded_model, model_save_dir / \"saved_qaunt.tmp\")\n",
    "\n",
    "learnerloaded = Learner(data, loaded_model, metrics=[dice], opt_func=torch.optim.SGD)\n",
    "learnermodel = Learner(data, model, metrics=[dice], opt_func=torch.optim.SGD)\n",
    "\n",
    "tester.print_loss(learnermodel,learnerloaded)\n",
    "tester.compare_size(model_save_dir / \"model.tmp\", model_save_dir / \"saved_qaunt.tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory and Run Speed\n",
    "While we were unable to implement it, it is entirely possible to never have to use floats at any point of the process. If you simply convert the entire network to just run off of integers you can replace the costly float multiplication operations. If you multiply two $2^{p}$s together you only need to calculate the sum of the exponenets.\n",
    "\n",
    "$2^{a}$ * $2^{b}$ = $2^{c}$\n",
    "\n",
    "and being that in this hypothetical we are only storing the exponents we can make it even simpler\n",
    "\n",
    "a + b = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "Another approach to optimize the neural net is to prune it. The pruning process can be done by following the steps below.\n",
    "\n",
    "1) Find a measure of importance and rank the filters accordingly\n",
    "\n",
    "2) Prune the weakest (lowest ranked) filter away.\n",
    "\n",
    "3) Retrain the network.\n",
    "\n",
    "4) Assess its performance.\n",
    "\n",
    "5) If performance starts to decrease, stop the process.\n",
    "\n",
    "These 5 steps can be performed in an iterative manner over the entire model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
